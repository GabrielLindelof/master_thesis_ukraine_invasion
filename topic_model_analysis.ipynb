{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Discourse and Emotions Around the Invasion of Ukraine - Companion code\n",
    "## – A Text Analytics Approach \n",
    "### Gabriel Lindelöf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspection and visualization of topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyarrow import feather\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "import os\n",
    "import pickle\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.options.display.float_format = '{:,.15f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset and pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feather.read_feather('data/ukraine_two_weeks_clean_shuffled_v2.feather', columns = ['id', 'text_clean', 'clean', 'created_at']) # load dataset                            \n",
    "dates = df.created_at.apply(lambda x: pd.Timestamp(x)).to_list() # change dates to supported format\n",
    "docs = df.text_clean.tolist() # clean documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.backend._utils import select_backend\n",
    "\n",
    "# load model with speciffied sentence transformer\n",
    "model_name = 'modelname'\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "topic_model = BERTopic.load('models/{}'.format(model_name), embedding_model=sentence_model)\n",
    "# load file containing topic labels for all documents (predicted in batches and then combined into single file)\n",
    "topics = pickle.load(open('models/{}_all.pickle'.format(model_name),'rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code that was used to combine the batches of predicted topic labels\n",
    "\n",
    "# topics2 =  pickle.load(open('models/modelname_batch_1-3.pickle','rb'))\n",
    "# topics3 =  pickle.load(open('models/modelname_batch_3-5.pickle','rb'))\n",
    "# topics4 =  pickle.load(open('models/modelname_batch_5-7.pickle','rb'))\n",
    "# topics5 =  pickle.load(open('models/modelname_batch_7-.pickle','rb'))\n",
    "\n",
    "# topics += topics2 + topics3 + topics4 + topics5\n",
    "# pd.Series(topics).value_counts()\n",
    "# print(len(topics), len(docs), len(dates))\n",
    "\n",
    "# print(\"SAVING TOPICS...\")\n",
    "\n",
    "# filename = 'models/modelname_all.pickle'\n",
    "# with open(filename, 'wb') as f:\n",
    "#     pickle.dump(topics, f)\n",
    "    \n",
    "# print(\"DONE TOPICS.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write some information about each topic to a collection of text files to aid analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def to_datetime(date):\n",
    "    date = datetime.fromisoformat(date[:-1])\n",
    "    return date\n",
    "\n",
    "df['date'] = df['created_at'].apply(to_datetime)\n",
    "df['date'] = df.date.apply(lambda x: x.date()) # convert time format\n",
    "\n",
    "docs_t = pd.DataFrame(zip(docs,topics, df.date), columns = [\"doc\", \"topic\", 'date'], index = df.id) # dataframe with the text, topic and date of each tweet\n",
    "\n",
    "\n",
    "for i in range(len(topic_model.get_topics())-1): # for each topic in model\n",
    "    topic = topic_model.get_representative_docs(i)[:20] # get representative docs for topic \n",
    "\n",
    "\n",
    "    with open('representative_docs/{:02d}_{}.txt'.format(i, topic_model.get_topic(i)[0][0]), 'w') as f: # save in text file, with keywords from topic and ID as name\n",
    "        f.write(\"Summary topic number {}\\n\\n\".format(i))\n",
    "\n",
    "        f.write(\"Topic frequency: {}\\n\\n\".format(topic_model.get_topic_freq(i))) \n",
    "\n",
    "        f.write(\"Top Words & TF-IDF Scores\\n\")\n",
    "        f.writelines([\"{}: {}\\n\".format(l[0], round(l[1], 4)) for l in topic_model.get_topic(i)])\n",
    "        f.write(\"\\n\\n\")\n",
    "        for index, doc in enumerate(topic):\n",
    "            f.write(\"[Document {}]\\n\".format(index+1))\n",
    "            f.write(doc)\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(\"1000 random documents from topic:\")\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        # samples 1000 docs from topic to write to bottom of text file\n",
    "        for row in docs_t[docs_t.topic == i].sample(1000).iterrows():\n",
    "            f.write(\"[{}] [{}]\\n\".format(row[0], row[1].date))\n",
    "            f.write(row[1].doc)\n",
    "            f.write(\"\\n____________________________________\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary visualizations of model (final versions were made manually with Matplotlib and Seaborn, further down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hie = topic_model.visualize_hierarchy()\n",
    "it = topic_model.visualize_topics()\n",
    "bar = topic_model.visualize_barchart(range(-1,len(topic_model.get_topics())-1))\n",
    "freq = topic_model.get_topic_freq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as webpages to allow sharing of interactive graphs\n",
    "hie.write_html(\"html/dendrogram.html\")\n",
    "it.write_html(\"html/intertopic_distance.html\")\n",
    "bar.write_html(\"html/topic_representations.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show graphs, and frequency table for topics\n",
    "hie.show()\n",
    "it.show()\n",
    "bar.show()\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a table in Words .docx format containing topics their frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame()\n",
    "table['Index'] = topic_model.get_topics().keys()\n",
    "table['Topic name'] = \"name\" # we add column with placeholder name, to be given by researcher based on key terms in topic\n",
    "\n",
    "# get top key-words for each topic\n",
    "key_words = []\n",
    "for x in topic_model.get_topics().values():\n",
    "    words = [tup[0] for tup in x][:7]\n",
    "    w_str = ', '.join(words)\n",
    "    key_words.append(w_str)\n",
    "\n",
    "table['Key terms'] = key_words\n",
    "table['Number of tweets'] = pd.Series(topics).value_counts().sort_index().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "from docx import Document\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# open an existing document\n",
    "doc = Document()\n",
    "\n",
    "doc.add_paragraph(\"Value Counts: \")\n",
    "\n",
    "t = doc.add_table(table.shape[0]+1, table.shape[1])\n",
    "t.style = 'Colorful List Accent 1'\n",
    "\n",
    "# add the header rows\n",
    "for j in range(table.shape[-1]):\n",
    "    t.cell(0,j).text = table.columns[j]\n",
    "\n",
    "# add the data from table\n",
    "for i in range(table.shape[0]):\n",
    "    for j in range(table.shape[-1]):\n",
    "        t.cell(i+1,j).text = str(table.values[i,j])\n",
    "        \n",
    "filename = \"topics_table.docx\"\n",
    "doc.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection of topics with word-clouds and info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dataframe with text, topic and creation date of tweets\n",
    "docs_t = pd.DataFrame(zip(docs[:len(topics)],topics, df.created_at), columns = [\"doc\", \"topic\", 'created_at'], index = df[:len(topics)].id)\n",
    "\n",
    "# load tokenized version of dataset\n",
    "tok = feather.read_feather('data/ukraine_two_weeks_clean_shuffled_v2_tok.feather')\n",
    "df = df.join(tok)\n",
    "\n",
    "# add topic info about each tweet to main dataset\n",
    "df['topic'] = topics\n",
    "df['topic_name'] = df.topic.apply(lambda x: topic_model.get_topic(x)[0][0])\n",
    "df['len'] = df.apply(len)\n",
    "topic_index_names = df[['topic', 'topic_name']].drop_duplicates().set_index('topic_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_topic(t, s = 0):\n",
    "    '''\n",
    "    Function that prints some info about a topic. \n",
    "    \n",
    "    Parameters:\n",
    "        t (int): index of topic\n",
    "        s (int): size of sample docs printed\n",
    "    '''\n",
    "    print(*topic_model.get_topic(t), '\\n', sep = '\\n') # prints top key-words\n",
    "    print('Representative docs:\\n')\n",
    "    if t != -1:\n",
    "        print(*topic_model.get_representative_docs(t)[:20], sep = '\\n__________________________________________________________________________________\\n\\n')\n",
    "    print('\\n')\n",
    "    s = docs_t[docs_t.topic == t].sample(s, random_state = 42).aggregate(lambda x: '{} |{}| ({})'.format(x.doc, x.created_at, x.topic), axis = 1) # sample from all docs in topic\n",
    "    print('Sample docs:', *s, sep = '\\n__________________________________________________________________________________\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize   \n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "\n",
    "class LemmaTokenizer:\n",
    "    '''Convert to lemmas for purpose of word clouds, class is based on https://gist.github.com/4OH4/f727af7dfc0e6bb0f26d2ea41d89ee55'''\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(str(t)) for t in doc if str(t)]\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    '''Remove numbers, we don't want them in the word clouds'''\n",
    "    text = re.sub(\" \\d+\", '', text)\n",
    "    return text\n",
    "\n",
    "def get_wordcloud(topic_i):\n",
    "    '''Get word cloud for topic with index passed as argument'''\n",
    "    \n",
    "    docs = df[(df.topic == topic_i)].tok # get tokenized version of all docs in topic\n",
    "\n",
    "    tokenizer=LemmaTokenizer()\n",
    "    stop_words = stopwords.words('english')\n",
    "    token_stop = tokenizer(stop_words)\n",
    "    lem_text = [' '.join(tokenizer(doc)) for doc in docs] # lemmatize documents \n",
    "\n",
    "    # vectorizer used to count word occurences\n",
    "    vectorizer = CountVectorizer(stop_words = token_stop, max_features = 200, lowercase = True, strip_accents = 'unicode', preprocessor=preprocess_text) \n",
    "    \n",
    "    # transform documents with vectorizer\n",
    "    transformed = vectorizer.fit_transform(lem_text)\n",
    "    \n",
    "    # get names (words) of each feature in vectorizer\n",
    "    fnames= vectorizer.get_feature_names() \n",
    "    \n",
    "    # combine word frequencies with corresponding words\n",
    "    freq_dict = dict(zip(vectorizer.get_feature_names(), np.asarray(transformed.sum(axis=0)).ravel()))\n",
    "\n",
    "    # initiate word cloud\n",
    "    wordcloud = WordCloud(max_words = 100, background_color=\"white\", colormap = 'tab10', width=1600, height=400, max_font_size=None, min_font_size = 0)\n",
    "    wordcloud.generate_from_frequencies(frequencies=freq_dict) # passing frequency dictonary to wordcloud\n",
    "    \n",
    "    plt.figure(figsize = (20,5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    filename = topic_index_names[topic_index_names.topic == topic_i].index[0]\n",
    "    plt.savefig(\"plots/wordclouds/{}_topic_{}.png\".format(filename, topic_i), format=\"png\", dpi = 300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 6 # supply with first keyword of topic OR index of topic\n",
    "\n",
    "# check if passed variable is str (name of topic) or int (index of topic), if str convert to index\n",
    "if isinstance(topic, str):\n",
    "    topic_i = int(topic_index_names.loc[topic][0])\n",
    "else:\n",
    "    topic_i = topic\n",
    "\n",
    "print(\"Getting topic with index: \", topic_i)   \n",
    "get_wordcloud(topic_i) # show wordcloud\n",
    "inspect_topic(topic_i, s = 100) # show info about topics content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # Reduce problems with cloud computing. \n",
    "\n",
    "\n",
    "\n",
    "coherence_results = []\n",
    "first_run = True\n",
    "for i in reversed(range(2, (len(set(topics))))): # Try every model size down until 2 topics. \n",
    "    if not first_run:\n",
    "        print(\"Reducing topics...\")\n",
    "        topics, _ = topic_model.reduce_topics(docs, topics, nr_topics=i) # Reduce with 1 topic. \n",
    " \n",
    "    # Dataframe with documents, their ID and topic. \n",
    "    documents = pd.DataFrame({\"Document\": docs,\n",
    "                              \"ID\": range(len(docs)),\n",
    "                              \"Topic\": topics})\n",
    "    documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "    \n",
    "    \n",
    "    print(\"Preprocessing...\")\n",
    "    cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values) # Get preprocessed texts.\n",
    "    \n",
    "    # Get topic models vectorizer and features. \n",
    "    vectorizer = topic_model.vectorizer_model \n",
    "    analyzer = vectorizer.build_analyzer() \n",
    "    words = vectorizer.get_feature_names() \n",
    "    \n",
    "    # Create a corpora of tokens. \n",
    "    tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "                   for topic in range(len(set(topics))-1)]\n",
    "\n",
    "    print(\"Calculating coherence...\")\n",
    "    # Calculate coherence for model. \n",
    "    coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                     texts=tokens, \n",
    "                                     corpus=corpus,\n",
    "                                     dictionary=dictionary, \n",
    "                                     coherence='c_v')\n",
    "    coherence = coherence_model.get_coherence()\n",
    "    \n",
    "    \n",
    "    coherence_results.append((i, coherence)) # Save number of topics and coherence. \n",
    "    print(i, coherence)\n",
    "    first_run = False\n",
    "    \n",
    "    with open('models/coherence_cv.pickle', 'wb') as f: # Write coherence results to file. \n",
    "        pickle.dump(coherence_results, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "coh = pickle.load(open('models/coherence_cv.pickle','rb')) # Load previous results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sns.set(rc={\"figure.dpi\":300, 'savefig.dpi':300})\n",
    "sns.set(rc={'figure.figsize':(2,2),\"font.size\":5,\"axes.titlesize\":5,\"axes.labelsize\":5, \"xtick.labelsize\" :5, \"ytick.labelsize\" :5})\n",
    "sns.set_style('whitegrid')\n",
    "ax = sns.lineplot(*zip(*reversed(coh2)))\n",
    "\n",
    "ax.set(xlabel = 'Number of topics', ylabel = 'C_V score')\n",
    "ax.axvline(15, color = 'red',linewidth=0.4, alpha = 0.5) # Add red line at selected number of topics. \n",
    "\n",
    "ax.figure.savefig('plots/coherence.png', bbox_inches=\"tight\", dpi = 300) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_over_time = topic_model.topics_over_time(docs, topics, dates, nr_bins=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/topics_over_time_14t.pickle' \n",
    "topics_over_time = pickle.load(open(filename,'rb')) # Load previously saved topics over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = topic_model.visualize_topics_over_time(topics_over_time, normalize_frequency = True) # Build in visualization for preliminary inspection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot.write_html(\"html/topics_over_time.html\") # Save to HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def to_datetime(date):\n",
    "    date = datetime.fromisoformat(date[:-1])\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "topics_i = [10] # Which topics to visualize (use index).\n",
    "topics = topics_over_time[topics_over_time.Topic.isin(topics_i)] # Select these topics. \n",
    "\n",
    "\n",
    "topic_strings = []\n",
    "# Add default labels, first keyword in each topic. \n",
    "for i in topics_i:\n",
    "    keywords = topic_model.get_topic(i)[:3]\n",
    "    string = \"{}: {}, {}, {}\".format(i, keywords[0][0], keywords[1][0], keywords[2][0])\n",
    "    topic_strings.append(string)\n",
    "\n",
    "## Override default names for visualiztion. \n",
    "#topic_strings = ['The invasion', 'NATO']\n",
    "#topic_strings = ['Foreign students', 'Cryptocurrency', 'Attacked cities']\n",
    "#topic_strings = ['Nazism', 'Energy', 'Planes', 'Biolabs']\n",
    "#topic_strings = ['China', 'Other conflicts', 'COVID-19', 'Solidarity']\n",
    "topic_strings =  ['Nuclear plants']\n",
    "sns.set(rc={\"figure.dpi\":300, 'savefig.dpi':300})\n",
    "\n",
    "\n",
    "sns.set(rc={'figure.figsize':(15,3),\"font.size\":15,\"axes.titlesize\":15,\"axes.labelsize\":15, \"xtick.labelsize\" :8, \"ytick.labelsize\" :10})\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "ax = sns.lineplot(data = topics, x = topics.Timestamp, y = topics['Frequency'], hue = topics.Topic, style = topics.Topic, palette = sns.color_palette('deep', len(topics_i)))\n",
    "\n",
    "ax.set(xlabel = 'Date', ylabel = 'Tweets')\n",
    "ax.legend(prop={'size': 10}, labels = topic_strings, loc = 'upper right')\n",
    "\n",
    "\n",
    "locator = mdates.DayLocator(interval=1)\n",
    "ax.xaxis.set_major_locator(locator)\n",
    "\n",
    "\n",
    "ax.set(ylim = (0,30000))\n",
    "ylabels = ['{:.0f}'.format(x) + 'k' for x in ax.get_yticks()/1000] # Reformat y-axis to count of thousands. \n",
    "ax.set_yticklabels(ylabels)\n",
    "ax.xaxis.labelpad = 15\n",
    "ax.yaxis.labelpad = 15\n",
    "\n",
    "ax.figure.savefig('plots/nuke.png', bbox_inches=\"tight\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (module anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
