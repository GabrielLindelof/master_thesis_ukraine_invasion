{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Discourse and Emotions Around the Invasion of Ukraine - Companion code\n",
    "## – A Text Analytics Approach \n",
    "### Gabriel Lindelöf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset cleaning and related plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import feather\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.options.display.float_format = '{:,.15f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = feather.read_feather('data/ukraine_two_weeks.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def to_datetime(date):\n",
    "    date = datetime.fromisoformat(date[:-1])\n",
    "    return date\n",
    "\n",
    "df['created_at'] = df['created_at'].apply(to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean(doc):\n",
    "    ''' Basic cleaning of raw text data. '''\n",
    "    doc = re.sub(r'http\\S+', '', doc) # remove any URLS\n",
    "    doc = re.sub(r'@\\S+', '', doc) # remove tagged @usernames \n",
    "    doc = re.sub(r'\\\\n', ' ', doc) # remove newlines\n",
    "    doc = re.sub(r'[\\s]+', ' ', doc) # remove extra spaces\n",
    "    return doc\n",
    "\n",
    "df['text_clean'] = df.text.apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creat variables to filter by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url'] = df['entities.urls'].isna() == False # mark if tweet contains URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['author.id'].value_counts().quantile(q=0.999) # mark if in upper 99.9 percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duplicate'] =  df.duplicated(keep = 'first', subset = ['text']) # mark if identical text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['duplicate_same_author'] =  df.duplicated(keep = 'first', subset = ['text', 'author.id'])  # mark if identical text content same author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts = df['author.id'].value_counts() # get n tweets each author\n",
    "print(\"99.9% percentile of tweeters in our dataset: \", val_counts.quantile(q=0.999)) \n",
    "print(val_counts.describe())\n",
    "val_counts_dict = val_counts.to_dict()\n",
    "df[\"n_tweets\"] = df['author.id'].apply(lambda x: val_counts_dict.get(x)) # add column indicating each tweets authors total n of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweets_999'] = df.n_tweets >= val_counts.quantile(q=0.999) # mark each tweet of users in the 99.9 percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter tweets by variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tweets_999.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts.quantile(q=0.999)\n",
    "print('After 99.9 quintile removal:')\n",
    "df.tweets_999.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Duplicate same author removal:')\n",
    "df.duplicate_same_author.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Duplicate removal:')\n",
    "df.duplicate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean'] = (df.duplicate == False) & (df.tweets_999 == False) # mark tweets matching criteria as clean, not to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before and after cleaning: ', df.clean.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.clean.sum() # validate final n of tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot before & after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and add data containing total number of tweets made matching query (not our dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.read_csv('data/counts.csv') # read file containing total number of tweets made matching query, gathered from Count endpoint\n",
    "total['start'] = total.start.apply(to_datetime) # convert to correct format\n",
    "total['hour'] = total.start.dt.hour # add hour\n",
    "total['date'] = total.start.dt.date # add date\n",
    "total = total[(total.start >= '2022-02-24') & (total.start < '2022-03-10')] # filter to make sure no tweets outside time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total: ', total.hour_count.sum())\n",
    "print('collected: ', len(df))\n",
    "print('clean: ', df.clean.sum())\n",
    "\n",
    "# Dataframe containing our tweets, as well as the total number of available tweets from Count endpoint. \n",
    "counts = pd.DataFrame({'Category': ['Available', 'Collected', 'Clean'], 'Tweets':[total.hour_count.sum(), len(df), df.clean.sum()]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot total number of tweets availabe, collected and after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={\"figure.dpi\":300, 'savefig.dpi':300})\n",
    "sns.set(rc={'figure.figsize':(2,2),\"font.size\":5,\"axes.titlesize\":5,\"axes.labelsize\":5, \"xtick.labelsize\" :5, \"ytick.labelsize\" :5})\n",
    "ax = sns.barplot(data = counts, x = 'Category', y = 'Tweets')\n",
    "ax.xaxis.labelpad = 5\n",
    "ax.yaxis.labelpad = 5\n",
    "ax.set(xlabel = '', ylabel = 'Tweets (millions)')\n",
    "\n",
    "ylabels = ['{:.0f}'.format(x) + 'm' for x in ax.get_yticks()/1000000] # format y-axis\n",
    "ax.set_yticklabels(ylabels)\n",
    "\n",
    "ax.figure.savefig('plots/counts.png', bbox_inches=\"tight\", dpi = 300) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot total number of tweets availabe, collected and after cleaning by hour of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_hour = total.groupby('hour').sum().rename(columns = {'hour_count':'Total tweets'}) # group counts of tweets by hour\n",
    "\n",
    "# add our dataset\n",
    "per_hour['Collected'] = df.created_at.dt.hour.value_counts().sort_index() \n",
    "per_hour['Clean'] = df.created_at[df.clean == True].dt.hour.value_counts().sort_index()\n",
    "\n",
    "# change to long format for easier plotting\n",
    "per_hour_l =pd.melt(per_hour, ignore_index=False)\n",
    "per_hour_l = per_hour_l.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15,3),\"font.size\":15,\"axes.titlesize\":15,\"axes.labelsize\":15, \"xtick.labelsize\" :10, \"ytick.labelsize\" :10})\n",
    "ax = sns.lineplot(data = per_hour_l, x = per_hour_l.hour, y = per_hour_l.value, hue = per_hour_l.variable, style = per_hour_l.variable)\n",
    "\n",
    "\n",
    "ax.set(xlabel = 'Hour of day', ylabel = 'Tweets (thousands)')\n",
    "\n",
    "ax.legend(loc = 'upper left', title = None)\n",
    "\n",
    "ylabels = ['{:.0f}'.format(x) + 'k' for x in ax.get_yticks()/1000] # format y-axis\n",
    "ax.set_yticklabels(ylabels)\n",
    "ax.xaxis.labelpad = 15\n",
    "ax.yaxis.labelpad = 15\n",
    "\n",
    "tickvalues = per_hour.index\n",
    "ax.set(xticks=tickvalues)\n",
    "\n",
    "ax.figure.savefig('plots/clean_hourofday.png', bbox_inches=\"tight\", dpi = 300) # plot total number of tweets per hour of day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot total number of tweets availabe, collected and after cleaning by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = total.drop('hour', axis = 1) # remove hours, no longer needed\n",
    "per_date = total.groupby(total.date).sum().rename(columns = {'hour_count':'Total tweets'}) # group count by day\n",
    "\n",
    "# add our dataset\n",
    "per_date['Collected'] = df.created_at.dt.date.value_counts().sort_index() \n",
    "per_date['Clean'] = df.created_at[df.clean == True].dt.date.value_counts().sort_index()\n",
    "\n",
    "# long format\n",
    "per_date_l =pd.melt(per_date, ignore_index=False)\n",
    "per_date_l = per_date_l.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_date['Total tweets'] # show total for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as md\n",
    "\n",
    "# date labels in more readable format are created manually\n",
    "datelab = []\n",
    "for i in range(24,29):\n",
    "    datelab.append('Feb {:2d}'.format(i))\n",
    "    \n",
    "for i in range(1,10):\n",
    "    datelab.append('Mar {:2d}'.format(i))\n",
    "    \n",
    "ax = sns.lineplot(data = per_date_l, x = per_date_l.date, y = per_date_l.value, hue = per_date_l.variable, style = per_date_l.variable)\n",
    "\n",
    "ax.set(xlabel = 'Date', ylabel = 'Tweets (millions)')\n",
    "ylabels = ['{:.1f}'.format(x) + 'm' for x in ax.get_yticks()/1000000] #format y-axis\n",
    "ax.set_yticklabels(ylabels)\n",
    "ax.legend(loc = 'upper right', title = None)\n",
    "ax.xaxis.labelpad = 15\n",
    "ax.yaxis.labelpad = 15\n",
    "\n",
    "tickvalues = per_date.index\n",
    "ax.set(xticks=tickvalues)\n",
    "ax.tick_params(axis='x', labelrotation=45) \n",
    "ax.set_xticklabels(datelab)\n",
    "\n",
    "ax.figure.savefig('plots/clean_per_day.png', bbox_inches=\"tight\", dpi = 300) # plot number of tweets per day Total, Collected and clean. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot total number of tweets availabe, collected and after cleaning with per-hour granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "# add columns with hour tweets were made\n",
    "df['date_hour'] = df['created_at'].apply(lambda x: x.replace(minute=0, second=0, microsecond=0) + timedelta(hours=1)) \n",
    "total['date_hour'] = total['start'].apply(lambda x: x.replace(minute=0, second=0, microsecond=0) + timedelta(hours=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_hour = total.groupby('date_hour').sum().rename(columns = {'hour_count':'Total tweets'}) # group counts by hour\n",
    "\n",
    "# group counts per hour for our dataset\n",
    "date_hour['Collected'] = df.date_hour.value_counts().sort_index()\n",
    "date_hour['Clean'] = df.date_hour[df.clean == True].value_counts().sort_index()\n",
    "\n",
    "# long format\n",
    "date_hour_l =pd.melt(date_hour, ignore_index=False)\n",
    "date_hour_l = date_hour_l.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lineplot(data = date_hour_l, x = date_hour_l.date_hour, y = date_hour_l.value, hue = date_hour_l.variable, style = date_hour_l.variable)\n",
    "\n",
    "ax.set(xlabel = 'Date & hour', ylabel = 'Tweets (thousands)')\n",
    "\n",
    "ax.xaxis.labelpad = 15\n",
    "ax.yaxis.labelpad = 15\n",
    "ylabels = ['{:.0f}'.format(x) + 'k' for x in ax.get_yticks()/1000] # format y-axis\n",
    "ax.set_yticklabels(ylabels)\n",
    "\n",
    "ax.legend(loc = 'upper right', title = None)\n",
    "\n",
    "tickvalues = per_date.index\n",
    "ax.set(xticks=tickvalues)\n",
    "ax.tick_params(axis='x', labelrotation=45)\n",
    "ax.set_xticklabels(datelab)\n",
    "\n",
    "ax.figure.savefig('plots/clean_per_date_hour.png', bbox_inches=\"tight\", dpi = 300) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (module anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
