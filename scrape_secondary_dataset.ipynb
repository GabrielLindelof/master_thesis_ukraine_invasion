{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Discourse and Emotions Around the Invasion of Ukraine - Companion code\n",
    "## – A Text Analytics Approach \n",
    "### Gabriel Lindelöf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape script secondary dataset - contagion hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twarc.client2 import Twarc2\n",
    "import pandas as pd\n",
    "import json \n",
    "from pyarrow import feather\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def to_datetime(date):\n",
    "    date = datetime.fromisoformat(date[:-1])\n",
    "    return date\n",
    "\n",
    "df = feather.read_feather('ukraine_two_weeks_clean_shuffled_v2_sample_10_10000.feather') # Load sampled users from main dataset.\n",
    "df['created_at'] = df['created_at'].apply(to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the users they are following - following endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Twarc2(bearer_token='secret_token_here') # Write Twitter API access token, initiate scrape object. \n",
    "\n",
    "\n",
    "from_user = -50 # Scraping was done in batches, start with user 0-50 (-50 since 50 is added start of loop)\n",
    "for group in range(20):\n",
    "    from_user += 50\n",
    "    to_user = from_user + 50 # 50 users at a time, then save before next loop. \n",
    "    print(\"From: \", from_user, \" to \", to_user)\n",
    "    \n",
    "    user_data = []\n",
    "    for user_i, user_id in enumerate(df.author_id.tolist()[from_user:to_user]):\n",
    "        print(\"Moving on to user: {} ({}/{})\".format(user_id, (user_i+1), (to_user - from_user)))\n",
    "        # Iterate over pages of followers\n",
    "        \n",
    "        page_df = next(t.following(user_id, max_results=1000)) # Get a maximum of 1000 followees for this user\n",
    "        try:\n",
    "            page_df = pd.json_normalize(page_df['data'], sep=',')\n",
    "            page_df['followed_by'] = user_id\n",
    "            user_data.append(page_df) # Save users followees to DataFrame.\n",
    "            print('Added {} followees to user_data list.'.format(len(page_df)))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"No data: \", e)\n",
    "\n",
    "    output = pd.concat(user_data)\n",
    "    feather.write_feather(output, 'data_out/following_{}_to_{}.feather'.format(from_user, to_user)) # Save batch of 50 users followees to file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get tweets made by followees - timeline endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = Twarc2(bearer_token='secret_token_here') # Write Twitter API access token, initiate scrape object. \n",
    "\n",
    "\n",
    "from_user = -50 # Getting tweets of followees was done in batches, start with the followees of user 0-50 (-50 since 50 is added start of loop)\n",
    "for group in range(20):\n",
    "    from_user += 50\n",
    "    to_user = from_user + 50 # Get first 50 users. \n",
    "    print(\"From: \", from_user, \" to \", to_user)\n",
    "    followees = feather.read_feather('data_out/following_{}_to_{}.feather'.format(from_user, to_user)) # Get the followees of these users. \n",
    "    \n",
    "    tweets = []\n",
    "    for user_i, user_id in enumerate(followees.followed_by.unique().tolist()):\n",
    "        ts_end = df[df.author_id == user_id].created_at.iloc[0] # when was response tweet created\n",
    "        ts_start = ts_end - timedelta(hours=3, minutes=0) # Only get tweets of followees made the preceeding 3 hours of response tweet. (later filtered to only 1 hour.)\n",
    "        ts_start = ts_start.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        ts_end = ts_end.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        \n",
    "        \n",
    "        user_followees = followees[followees.followed_by == user_id] # Get all the followees of the 50 users. \n",
    "        print(\"Getting {} users that {} follows ({})\".format(len(user_followees), user_id, user_i))\n",
    "        for followed_user in user_followees.id: # iterate all users they follow\n",
    "            try: \n",
    "                # Query if their timelines contain any tweets the time before the response tweet. \n",
    "                query = t.timeline(followed_user,  start_time=ts_start, end_time=ts_end, exclude_retweets=False, exclude_replies=True, max_results=100, expansions=None, tweet_fields=None, user_fields=None)\n",
    "                followed_timeline = next(query)\n",
    "                followed_timeline = pd.json_normalize(followed_timeline['data'], sep=',')\n",
    "                followed_timeline['followed_by'] = user_id\n",
    "                print(\"Adding tweets: \", len(followed_timeline))\n",
    "                tweets.append(followed_timeline) # Save any tweets from followees timeline\n",
    "                \n",
    "                \n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    output = pd.concat(tweets)\n",
    "    feather.write_feather(output, 'data_out/follow_tweets{}_to_{}.feather'.format(from_user, to_user)) # Save data to file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (module anaconda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
